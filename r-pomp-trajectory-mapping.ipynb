{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4594a84a-a5ac-43c5-b9c2-d7b4cb253998",
   "metadata": {},
   "source": [
    "## Building the application\n",
    "\n",
    "This step follows a similar set of commands as what exists in the README. It builds the Dockerfile that exists in directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248134c",
   "metadata": {},
   "source": [
    "## First login by running `az login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832989c-a906-4edc-9c51-e304f8eabe0e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "!./build-image.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b79c84",
   "metadata": {},
   "source": [
    "# Test your code\n",
    "\n",
    "We mount our app directory on the docker container and run our `generate_ofun.R` code.  This generates our optimization function `ofun.rds` a one-time step that we don't wish to repeat in every batch task.\n",
    "Then we do the same volume mount, and run our `run_optimization.R` script with `K` and `N_0` input params. This runs our objective function using `subplex` to optimize the input parameters. This is what will be run as tasks on batch, with varying input parameter spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe013b30",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "!docker run --rm \\\n",
    "    -v \"$(pwd)/app:/app\" \\\n",
    "    <azureContainerRegistryName>.azurecr.io/r-pomp:4.4.1 Rscript /app/generate_ofun.R\n",
    "    \n",
    "!docker run --rm \\\n",
    "    -v \"$(pwd)/app:/app\" \\\n",
    "    <azureContainerRegistryName>.azurecr.io/r-pomp:4.4.1 Rscript /app/run_optimization.R 1500 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f2fe28-27a9-41ce-bc54-cc92bef98901",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The following four cells do the required work before we actually start interacting with Batch.\n",
    "- Install the required libraries in your python environment\n",
    "- Import all the required libraries. You can view those in the `requirements.txt` in this directory.\n",
    "- Put the required configuration into memory, pulling any sensitive information out of environment variables\n",
    "- Create the requisite client and configuration objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc146d2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3fb130-10b7-496a-a612-0c0d86f6b3ad",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import azure.batch as batch\n",
    "from azure.storage.blob import ContainerClient\n",
    "from azure.identity import  DefaultAzureCredential\n",
    "from msrest.authentication import BasicTokenAuthentication\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c88c67a3-1baa-4652-9984-8c522245b245",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "# Batch configuration\n",
    "_BATCH_ACCOUNT_URL = 'https://<batchAccountName>.<region>.batch.azure.com'\n",
    "_BATCH_ACCOUNT_MANAGED_ID_RESOURCE_ID=\"/subscriptions/<subscriptionId>/resourcegroups/<resourceGroupName>/providers/Microsoft.ManagedIdentity/userAssignedIdentities/<batchAccountManagedIdentity>\"\n",
    "\n",
    "_JOB_ID = f'r-pomp-trajectory-mapping-{timestamp}'\n",
    "_POOL_ID = 'default'\n",
    "\n",
    "# Storage configuration\n",
    "_STORAGE_ACCOUNT_NAME = '<storageAccountName>'\n",
    "_CONTAINER_NAME = 'output'\n",
    "_ACCOUNT_URL = f'https://{_STORAGE_ACCOUNT_NAME}.blob.core.windows.net'\n",
    "\n",
    "# for usage with sas keys enabled and not using user managed identity\n",
    "# _OUTPUT_CONTAINER_URL = f'{_ACCOUNT_URL}/{_CONTAINER_NAME}{_OUTPUT_CONTAINER_SAS}'\n",
    "_OUTPUT_CONTAINER_URL = f'{_ACCOUNT_URL}/{_CONTAINER_NAME}'\n",
    "\n",
    "# ACR configuration\n",
    "_ACR_SERVER='<azureContainerRegistryName>.azurecr.io'\n",
    "_IMAGE = f'{_ACR_SERVER}/r-pomp:4.4.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b731b55-27f2-49f3-822c-6259a0f6a973",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# credentials\n",
    "\n",
    "default_credential = DefaultAzureCredential()\n",
    "identity_reference = batch.models.ComputeNodeIdentityReference(resource_id=_BATCH_ACCOUNT_MANAGED_ID_RESOURCE_ID)\n",
    "\n",
    "token = {'access_token': default_credential.get_token('https://batch.core.windows.net/.default').token}\n",
    "batch_credentials = BasicTokenAuthentication(token)\n",
    "\n",
    "# service clients\n",
    "\n",
    "batch_client = batch.BatchServiceClient(batch_credentials, batch_url=_BATCH_ACCOUNT_URL)\n",
    "blob_service_client = BlobServiceClient(account_url=_ACCOUNT_URL, credential=default_credential)\n",
    "\n",
    "input_container_client = blob_service_client.get_container_client('input')\n",
    "output_container_client = ContainerClient(account_url=_ACCOUNT_URL, container_name=_CONTAINER_NAME, credential=default_credential)\n",
    "\n",
    "# configuration objects\n",
    "task_registry = batch.models.ContainerRegistry(registry_server=_ACR_SERVER, identity_reference=identity_reference)\n",
    "task_container_settings = batch.models.TaskContainerSettings(image_name=_IMAGE, registry=task_registry)\n",
    "environment_settings = [batch.models.EnvironmentSetting(name=k, value=v) for k, v in {}.items()]\n",
    "user = batch.models.UserIdentity(auto_user=batch.models.AutoUserSpecification(elevation_level=batch.models.ElevationLevel.admin, scope=batch.models.AutoUserScope.task))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be432708",
   "metadata": {},
   "source": [
    "# Upload the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c60e59",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def upload_input_files(file_paths):\n",
    "\n",
    "    print(f'Found {len(file_paths)} files to upload.')\n",
    "\n",
    "    print(f'Uploading files to container input/{_JOB_ID}...')\n",
    "    for file_path in file_paths:\n",
    "        print(f'Uploading {file_path}...')\n",
    "        with open(file_path, 'rb') as f:\n",
    "            input_container_client.upload_blob(name=f'{_JOB_ID}/{os.path.basename(file_path)}', data=f, overwrite=True)\n",
    "\n",
    "    print('Upload complete.')\n",
    "\n",
    "args = ['./app/ofun.rds','./app/run_optimization.R']\n",
    "\n",
    "upload_input_files(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c26a4-8761-423c-9cf6-fa9176b015d5",
   "metadata": {},
   "source": [
    "# Creating the job\n",
    "\n",
    "The Batch job itself is relatively simple at its core. All it needs is a pool and an id. There are more things that can be configured, such as preparation and completion tasks or behavior when a task fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "204dd8a4-77ea-4b62-a115-e091ee1b6768",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "job = batch.models.JobAddParameter(id=_JOB_ID, pool_info=batch.models.PoolInformation(pool_id=_POOL_ID))\n",
    "batch_client.job.add(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adab2dd-7bb5-4128-8f22-47795880d6fe",
   "metadata": {},
   "source": [
    "## Creating the tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a3ab8b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import azure.batch as batch\n",
    "\n",
    "# Define the parameter spaces for the three tasks\n",
    "parameter_initial_values = [\n",
    "    {\"K\": 100, \"N_0\": 10},\n",
    "    {\"K\": 200, \"N_0\": 20},\n",
    "    {\"K\": 300, \"N_0\": 30}\n",
    "]\n",
    "\n",
    "tasks = []\n",
    "\n",
    "for i, params in enumerate(parameter_initial_values):\n",
    "    task_name = f\"task_{i+1}\"\n",
    "    output_name = f'{_JOB_ID}/{task_name}'\n",
    "    command = f'/bin/bash -c \"cd {_JOB_ID} && chmod +x run_optimization.R && Rscript ./run_optimization.R  {params[\"K\"]} {params[\"N_0\"]}\"'\n",
    "\n",
    "    task = batch.models.TaskAddParameter(\n",
    "        id=task_name,\n",
    "        command_line=command,\n",
    "        container_settings=task_container_settings,\n",
    "        environment_settings=environment_settings,\n",
    "        user_identity=user,\n",
    "        resource_files=[\n",
    "            batch.models.ResourceFile(\n",
    "                auto_storage_container_name='input',\n",
    "                blob_prefix=f'{_JOB_ID}/'\n",
    "            )\n",
    "        ],\n",
    "        output_files=[\n",
    "            batch.models.OutputFile(\n",
    "                file_pattern='../std*.txt',\n",
    "                destination=batch.models.OutputFileDestination(\n",
    "                    container=batch.models.OutputFileBlobContainerDestination(\n",
    "                        path=f'{output_name}/logs',\n",
    "                        container_url=f'{_ACCOUNT_URL}/output',\n",
    "                        identity_reference=identity_reference,\n",
    "                        upload_headers=[batch.models.HttpHeader(name=\"Metadata\", value=\"true\")]\n",
    "                    )\n",
    "                ),\n",
    "                upload_options=batch.models.OutputFileUploadOptions(\n",
    "                    upload_condition=batch.models.OutputFileUploadCondition.task_completion)\n",
    "            ),\n",
    "            batch.models.OutputFile(\n",
    "                file_pattern=f'./{_JOB_ID}/**/*',\n",
    "                destination=batch.models.OutputFileDestination(\n",
    "                    container=batch.models.OutputFileBlobContainerDestination(\n",
    "                        path=f'{output_name}/data',\n",
    "                        container_url=f'{_ACCOUNT_URL}/output',\n",
    "                        identity_reference=identity_reference,\n",
    "                        upload_headers=[batch.models.HttpHeader(name=\"Metadata\", value=\"true\")]\n",
    "                    )\n",
    "                ),\n",
    "                upload_options=batch.models.OutputFileUploadOptions(\n",
    "                    upload_condition=batch.models.OutputFileUploadCondition.task_success)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    tasks.append(task)\n",
    "\n",
    "print(f'Firing off {len(tasks)} tasks!')\n",
    "\n",
    "result = batch_client.task.add_collection(_JOB_ID, tasks)\n",
    "\n",
    "batch_client.job.patch(_JOB_ID, batch.models.JobPatchParameter(on_all_tasks_complete=batch.models.OnAllTasksComplete.terminate_job))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f8b1a8-1289-4744-876c-be3cce10790f",
   "metadata": {},
   "source": [
    "## Creating an artifact\n",
    "\n",
    "Now that our simulation tasks have all finished, we have a bunch of disparate files in Azure Storage. These steps download each of the files within the container path, load them into a MultiSim object that exists on this notebook host, and finally creates a plot out of the aggregated simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8475a2e4-01d1-4887-8c07-6d20a690234a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get the list of simulations that completed\n",
    "simulations = [b.name for b in output_container_client.list_blobs()]\n",
    "\n",
    "# Filter and print only those simulations in the specified folder\n",
    "for s in simulations:\n",
    "    if s.startswith(_JOB_ID):\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa23552-47ef-43e0-8e8a-f50559d49dfe",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the list of .RData files in the specified folder\n",
    "rdata_files = [b.name for b in output_container_client.list_blobs() if b.name.startswith(_JOB_ID) and b.name.endswith('.RData')]\n",
    "\n",
    "# Download the .RData files\n",
    "if not os.path.exists('downloaded_data'):\n",
    "    os.makedirs('downloaded_data')\n",
    "\n",
    "for rdata_file in rdata_files:\n",
    "    blob_client = blob_service_client.get_blob_client(container=_CONTAINER_NAME, blob=rdata_file)\n",
    "    file_name = os.path.join('downloaded_data', os.path.basename(rdata_file))\n",
    "    with open(file_name, \"wb\") as download_file:\n",
    "        download_file.write(blob_client.download_blob().readall())\n",
    "\n",
    "print(f\"Downloaded {len(rdata_files)} .RData files to 'downloaded_data' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c2c00",
   "metadata": {},
   "source": [
    "# Switch to R kernel\n",
    "\n",
    "Switch your jupyter notebook to an R kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7230a9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "install.packages('pomp')\n",
    "install.packages('ggplot2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e42d5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "\n",
    "# Directory where the .RData files are stored\n",
    "data_dir <- \"downloaded_data\"\n",
    "\n",
    "# List of RData files with full paths\n",
    "rdata_files <- list.files(data_dir, pattern = \"fit_results_.*\\\\.RData\", full.names = TRUE)\n",
    "\n",
    "# Initialize an empty data frame to store the results\n",
    "results_df <- data.frame(neg_log_likelihood = numeric(), K = numeric(), N_0 = numeric(), file = character())\n",
    "\n",
    "# Loop through the RData files to extract and store the final K and N_0\n",
    "for (file in rdata_files) {\n",
    "  # Load the fit object from the RData file\n",
    "  load(file)\n",
    "  \n",
    "  # Extract the final optimized K and N_0\n",
    "  final_K <- fit$par[\"K\"]\n",
    "  final_N_0 <- fit$par[\"N_0\"]\n",
    "  neg_log_likelihood <- fit$value\n",
    "\n",
    "  # Append the results to the data frame\n",
    "  results_df <- rbind(results_df, data.frame(neg_log_likelihood = neg_log_likelihood, K = final_K, N_0 = final_N_0, file = basename(file)))\n",
    "}\n",
    "rownames(results_df) <- NULL\n",
    "\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
