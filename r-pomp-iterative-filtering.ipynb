{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4594a84a-a5ac-43c5-b9c2-d7b4cb253998",
   "metadata": {},
   "source": [
    "## Building the application\n",
    "\n",
    "This step follows a similar set of commands as what exists in the README. It builds the Dockerfile that exists in directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248134c",
   "metadata": {},
   "source": [
    "## First login by running `az login --use-device-code` in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832989c-a906-4edc-9c51-e304f8eabe0e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "!./build-and-push-image.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b79c84",
   "metadata": {},
   "source": [
    "# Test your code and generate artifacts\n",
    "\n",
    "You can manually change your guesses parameter space by changing the sobol_design upper and lower bounds in [generate_guesses_vpc.R](./app/generate_guesses_vpc.R). Next, define your numguesses in the jupyter python context.below.\n",
    "\n",
    "We mount our `/app` directory on the docker container and run our `generate_guesses_vpc.R` code with `numguesses` as an input argument. This generates our guesses and our vpC pomp model object.  \n",
    "\n",
    "Then we do the same volume mount, and run our `run_mif2_guesses.R` script with 1 as the input argument. This runs mif2 on the first guess. This is what will be run as tasks on batch, with each guess representing a different starting point in the input parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe013b30",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "numguesses = 1000 # <--- SET YOUR numguesses\n",
    "\n",
    "!docker run --rm \\\n",
    "    -v \"$(pwd)/app:/app\" \\\n",
    "    <azureContainerRegistryName>.azurecr.io/r-pomp:4.4.1 Rscript /app/generate_guesses_vpc.R {numguesses}\n",
    "    \n",
    "!docker run --rm \\\n",
    "    -v \"$(pwd)/app:/app\" \\\n",
    "    <azureContainerRegistryName>.azurecr.io/r-pomp:4.4.1 Rscript /app/run_mif2_guesses.R 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f2fe28-27a9-41ce-bc54-cc92bef98901",
   "metadata": {},
   "source": [
    "## Azure Batch Setup\n",
    "\n",
    "The following three cells do the required work before we actually start interacting with Batch.\n",
    "- Import all the required libraries. You can view those in the `requirements.txt` in this directory.\n",
    "- Put the required configuration into memory, pulling any sensitive information out of environment variables\n",
    "- Create the requisite client and configuration objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e56e1",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3fb130-10b7-496a-a612-0c0d86f6b3ad",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import azure.batch as batch\n",
    "from azure.storage.blob import ContainerClient\n",
    "from azure.identity import  DefaultAzureCredential\n",
    "from msrest.authentication import BasicTokenAuthentication\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c88c67a3-1baa-4652-9984-8c522245b245",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "# Batch configuration\n",
    "_BATCH_ACCOUNT_URL = 'https://<batchAccountName>.<region>.batch.azure.com'\n",
    "_BATCH_ACCOUNT_MANAGED_ID_RESOURCE_ID=\"/subscriptions/<subscriptionId>/resourcegroups/<resourceGroupName>/providers/Microsoft.ManagedIdentity/userAssignedIdentities/<batchAccountManagedIdentity>\"\n",
    "\n",
    "_JOB_ID = f'r-pomp-iterative-filtering-{timestamp}'\n",
    "_POOL_ID = 'default'\n",
    "\n",
    "# Storage configuration\n",
    "_STORAGE_ACCOUNT_NAME = '<storageAccountName>'\n",
    "_CONTAINER_NAME = 'output'\n",
    "_ACCOUNT_URL = f'https://{_STORAGE_ACCOUNT_NAME}.blob.core.windows.net'\n",
    "\n",
    "# for usage with sas keys enabled and not using user managed identity\n",
    "# _OUTPUT_CONTAINER_URL = f'{_ACCOUNT_URL}/{_CONTAINER_NAME}{_OUTPUT_CONTAINER_SAS}'\n",
    "_OUTPUT_CONTAINER_URL = f'{_ACCOUNT_URL}/{_CONTAINER_NAME}'\n",
    "\n",
    "# ACR configuration\n",
    "_ACR_SERVER='<azureContainerRegistryName>.azurecr.io'\n",
    "_IMAGE = f'{_ACR_SERVER}/r-pomp:4.4.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b731b55-27f2-49f3-822c-6259a0f6a973",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# credentials\n",
    "\n",
    "default_credential = DefaultAzureCredential()\n",
    "identity_reference = batch.models.ComputeNodeIdentityReference(resource_id=_BATCH_ACCOUNT_MANAGED_ID_RESOURCE_ID)\n",
    "\n",
    "token = {'access_token': default_credential.get_token('https://batch.core.windows.net/.default').token}\n",
    "batch_credentials = BasicTokenAuthentication(token)\n",
    "\n",
    "# service clients\n",
    "\n",
    "batch_client = batch.BatchServiceClient(batch_credentials, batch_url=_BATCH_ACCOUNT_URL)\n",
    "blob_service_client = BlobServiceClient(account_url=_ACCOUNT_URL, credential=default_credential)\n",
    "\n",
    "input_container_client = blob_service_client.get_container_client('input')\n",
    "output_container_client = ContainerClient(account_url=_ACCOUNT_URL, container_name=_CONTAINER_NAME, credential=default_credential)\n",
    "\n",
    "# configuration objects\n",
    "task_registry = batch.models.ContainerRegistry(registry_server=_ACR_SERVER, identity_reference=identity_reference)\n",
    "task_container_settings = batch.models.TaskContainerSettings(image_name=_IMAGE, registry=task_registry)\n",
    "environment_settings = [batch.models.EnvironmentSetting(name=k, value=v) for k, v in {}.items()]\n",
    "user = batch.models.UserIdentity(auto_user=batch.models.AutoUserSpecification(elevation_level=batch.models.ElevationLevel.admin, scope=batch.models.AutoUserScope.task))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be432708",
   "metadata": {},
   "source": [
    "# Upload the files\n",
    "\n",
    "Upload the required files to the Azure Storage input container, which will be available to your batch tasks when they run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c60e59",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def upload_input_files(file_paths):\n",
    "\n",
    "    print(f'Found {len(file_paths)} files to upload.')\n",
    "\n",
    "    print(f'Uploading files to container input/{_JOB_ID}...')\n",
    "    for file_path in file_paths:\n",
    "        print(f'Uploading {file_path}...')\n",
    "        with open(file_path, 'rb') as f:\n",
    "            input_container_client.upload_blob(name=f'{_JOB_ID}/{os.path.basename(file_path)}', data=f, overwrite=True)\n",
    "\n",
    "    print('Upload complete.')\n",
    "\n",
    "args = ['./app/guesses.rds', './app/vpC.rds', './app/run_mif2_guesses.R']\n",
    "\n",
    "upload_input_files(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c26a4-8761-423c-9cf6-fa9176b015d5",
   "metadata": {},
   "source": [
    "# Creating the job\n",
    "\n",
    "The Batch job itself is relatively simple at its core. All it needs is a pool and an id. There are more things that can be configured, such as preparation and completion tasks or behavior when a task fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "204dd8a4-77ea-4b62-a115-e091ee1b6768",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "job = batch.models.JobAddParameter(id=_JOB_ID, pool_info=batch.models.PoolInformation(pool_id=_POOL_ID,))\n",
    "batch_client.job.add(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adab2dd-7bb5-4128-8f22-47795880d6fe",
   "metadata": {},
   "source": [
    "## Creating the tasks\n",
    "\n",
    "Create your tasks to be dispatched to your job and run.  Each task will run a mif2 guess and save the results in an RData object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a3ab8b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import azure.batch as batch\n",
    "\n",
    "tasks = []\n",
    "\n",
    "for i in range(1, numguesses + 1):\n",
    "    task_name = f\"task_{i}\"\n",
    "    output_name = f'{_JOB_ID}/{task_name}'\n",
    "    command = f'/bin/bash -c \"cd {_JOB_ID} && chmod +x run_mif2_guesses.R && Rscript ./run_mif2_guesses.R {i}\"'\n",
    "\n",
    "    task = batch.models.TaskAddParameter(\n",
    "        id=task_name,\n",
    "        command_line=command,\n",
    "        container_settings=task_container_settings,\n",
    "        environment_settings=environment_settings,\n",
    "        user_identity=user,\n",
    "        resource_files=[\n",
    "            batch.models.ResourceFile(\n",
    "                auto_storage_container_name='input',\n",
    "                blob_prefix=f'{_JOB_ID}/'\n",
    "            )\n",
    "        ],\n",
    "        output_files=[\n",
    "            batch.models.OutputFile(\n",
    "                file_pattern='../std*.txt',\n",
    "                destination=batch.models.OutputFileDestination(\n",
    "                    container=batch.models.OutputFileBlobContainerDestination(\n",
    "                        path=f'{output_name}/logs',\n",
    "                        container_url=f'{_ACCOUNT_URL}/output',\n",
    "                        identity_reference=identity_reference,\n",
    "                        upload_headers=[batch.models.HttpHeader(name=\"Metadata\", value=\"true\")]\n",
    "                    )\n",
    "                ),\n",
    "                upload_options=batch.models.OutputFileUploadOptions(\n",
    "                    upload_condition=batch.models.OutputFileUploadCondition.task_completion)\n",
    "            ),\n",
    "            batch.models.OutputFile(\n",
    "                file_pattern=f'./{_JOB_ID}/**/*',\n",
    "                destination=batch.models.OutputFileDestination(\n",
    "                    container=batch.models.OutputFileBlobContainerDestination(\n",
    "                        path=f'{output_name}/data',\n",
    "                        container_url=f'{_ACCOUNT_URL}/output',\n",
    "                        identity_reference=identity_reference,\n",
    "                        upload_headers=[batch.models.HttpHeader(name=\"Metadata\", value=\"true\")]\n",
    "                    )\n",
    "                ),\n",
    "                upload_options=batch.models.OutputFileUploadOptions(\n",
    "                    upload_condition=batch.models.OutputFileUploadCondition.task_success)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    tasks.append(task)\n",
    "\n",
    "print(f'Firing off {len(tasks)} tasks!')\n",
    "\n",
    "result = batch_client.task.add_collection(_JOB_ID, tasks)\n",
    "\n",
    "batch_client.job.patch(_JOB_ID, batch.models.JobPatchParameter(on_all_tasks_complete=batch.models.OnAllTasksComplete.terminate_job))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f8b1a8-1289-4744-876c-be3cce10790f",
   "metadata": {},
   "source": [
    "## Creating an artifact\n",
    "\n",
    "Now that our simulation tasks have all finished, we have a bunch of disparate files in Azure Storage. These steps download each of the files within the container path, load them into our local environment, and run some analysis on them in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8475a2e4-01d1-4887-8c07-6d20a690234a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get the list of simulations that completed\n",
    "blobs = [b.name for b in output_container_client.list_blobs()]\n",
    "\n",
    "# Filter and print only those blobs in the specified folder\n",
    "for s in blobs[100:]:\n",
    "    if s.startswith(_JOB_ID):\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa23552-47ef-43e0-8e8a-f50559d49dfe",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the list of .RData files in the specified folder\n",
    "rdata_files = [b.name for b in output_container_client.list_blobs() if b.name.startswith(_JOB_ID) and b.name.endswith('.RData')]\n",
    "\n",
    "# Download the .RData files\n",
    "if not os.path.exists('downloaded_data'):\n",
    "    os.makedirs('downloaded_data')\n",
    "\n",
    "for rdata_file in rdata_files:\n",
    "    blob_client = blob_service_client.get_blob_client(container=_CONTAINER_NAME, blob=rdata_file)\n",
    "    file_name = os.path.join('downloaded_data', os.path.basename(rdata_file))\n",
    "    with open(file_name, \"wb\") as download_file:\n",
    "        download_file.write(blob_client.download_blob().readall())\n",
    "\n",
    "print(f\"Downloaded {len(rdata_files)} .RData files to 'downloaded_data' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06264960",
   "metadata": {},
   "source": [
    "## Switch to R kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7230a9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "install.packages('pomp')\n",
    "install.packages('ggplot2')\n",
    "install.packages('tidyverse')\n",
    "install.packages('reshape2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e42d5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(pomp)\n",
    "library(ggplot2)\n",
    "library(tidyverse)\n",
    "library(reshape2)\n",
    "\n",
    "# Directory where the .RData files are stored\n",
    "data_dir <- \"downloaded_data\"\n",
    "guesses <- readRDS(paste0(\"./app/guesses.rds\"))\n",
    "\n",
    "\n",
    "# Traces plot\n",
    "# List of RData files with full paths\n",
    "mif_rdata_files <- list.files(data_dir, pattern = \"mif_result_.*\\\\.RData\", full.names = TRUE)\n",
    "\n",
    "# Initialize an empty list to store the mif2d_pomp objects\n",
    "mifs_list <- list()\n",
    "\n",
    "# Load each RData file and extract the mif2d_pomp object\n",
    "for (file in mif_rdata_files) {\n",
    "  load(file)\n",
    "  mifs_list <- c(mifs_list, list(mif_result))  # Assuming each RData file contains an object named 'mif_result'\n",
    "}\n",
    "\n",
    "# Generate the ggplot\n",
    "trace_plot <- mifs_list |>\n",
    "  lapply(traces) |>\n",
    "  lapply(as.data.frame) |>\n",
    "  lapply(function(df) {\n",
    "    df <- df %>% mutate(iteration = row_number())\n",
    "    return(df)\n",
    "  }) |>\n",
    "  bind_rows(.id = \"guess\") |>\n",
    "  reshape2::melt(id.vars = c(\"iteration\", \"guess\")) |>\n",
    "  filter(variable != \"b\") |>\n",
    "  ggplot(aes(x = iteration, y = value, group = guess, color = factor(guess))) +\n",
    "  geom_line(size = 0.7, alpha = 0.7) +  # Thinner lines with transparency\n",
    "  facet_wrap(~ variable, scales = \"free_y\") +\n",
    "  guides(color = \"none\") +\n",
    "  labs(x = \"Iteration\", y = \"Value\", title = \"Traces from mif2 runs\")\n",
    "\n",
    "# Save the ggplot to a PNG file\n",
    "ggsave(\"./results/trace_plot.png\", plot = trace_plot, width = 15, height = 12, dpi = 300)\n",
    "\n",
    "\n",
    "# Generate pair plot\n",
    "pfilter_rdata_files <- list.files(data_dir, pattern = \"mif_pfilter_result_.*\\\\.RData\", full.names = TRUE)\n",
    "\n",
    "# Initialize an empty list to store the mif2d_pomp objects\n",
    "mifs_pfilter_list <- list()\n",
    "\n",
    "# Load each RData file and extract the mif2d_pomp object\n",
    "for (file in pfilter_rdata_files) {\n",
    "  load(file)\n",
    "  mifs_pfilter_list <- c(mifs_pfilter_list, list(pfilter_result))\n",
    "}\n",
    "\n",
    "# Extract log likelihoods from each pfilter result\n",
    "log_lik_values <- sapply(mifs_pfilter_list, logLik)\n",
    "\n",
    "log_lik_df <- data.frame(\n",
    "  .id = seq_len(nrow(guesses)),\n",
    "  loglik = log_lik_values\n",
    ")\n",
    "\n",
    "# Extract coefficients from each mif2d_pomp object\n",
    "coef_list <- lapply(mifs_list, coef)\n",
    "coef_df <- bind_rows(coef_list)\n",
    "coef_df$.id <- seq_along(coef_list)\n",
    "\n",
    "# Reshape coefficients data frame\n",
    "coef_df_long <- pivot_longer(coef_df, cols = -\".id\")\n",
    "\n",
    "# Combine log likelihoods and coefficients\n",
    "estimates <- left_join(log_lik_df, coef_df_long, by = \".id\")\n",
    "\n",
    "# Reshape the data to wide format\n",
    "estimates_wide <- pivot_wider(estimates, names_from = name, values_from = value)\n",
    "\n",
    "# Generate the pairs plot and save it to a PNG file\n",
    "png(\"./results/pairs_plot.png\", width = 3000, height = 2400, res = 450)\n",
    "\n",
    "estimates_wide |>\n",
    "  bind_rows(guesses) |>\n",
    "  filter(is.na(loglik) | loglik > max(loglik, na.rm = TRUE) - 30) |>\n",
    "  mutate(col = if_else(is.na(loglik), \"#99999955\", \"#ff0000ff\")) |>\n",
    "  {\n",
    "    \\(dat) pairs(\n",
    "      ~loglik + r + sigma + K + N_0,\n",
    "      data = dat,\n",
    "      col = dat$col,\n",
    "      pch = 16,\n",
    "      cex = 0.6,\n",
    "      cex.labels = 0.8\n",
    "    )\n",
    "  }()\n",
    "\n",
    "dev.off()  # Ensure the device is properly closed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
